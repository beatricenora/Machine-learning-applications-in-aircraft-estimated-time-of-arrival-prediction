# import libraries
import pandas as pd
import numpy as np
import glob
import logging
import gc
import warnings
from google.colab import drive
from pyproj import Geod
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
!pip install scikit-learn==1.2.2 lightgbm==3.3.2

# logging set up
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# function to calculate Haversine distance
def haversine_distance(lat1, lon1, lat2, lon2):
    geod = Geod(ellps="WGS84")
    distances = []
    for la1, lo1, la2, lo2 in zip(lat1, lon1, lat2, lon2):
        _, _, distance = geod.inv(lo1, la1, lo2, la2)
        distances.append(distance / 1852)  # Conversione in miglia nautiche
    return distances

def process_flight_data(df, airport_lat, airport_lon):
    """Process individual flight data with error handling."""
    try:
        df = df.sort_values('time')
        df['distance_to_airport'] = haversine_distance(
            df['lat'].values, df['lon'].values,
            [airport_lat]*len(df), [airport_lon]*len(df)
        )

        # Find entry point (48-50 nautical miles from airport)
        entry_mask = (df['distance_to_airport'] <= 50) & (df['distance_to_airport'] >= 48)
        entry_index = df[entry_mask].index.min()

        if pd.isna(entry_index):
            return None

        entry_row = df.loc[entry_index]
        lowest_baroaltitude = df['baroaltitude'].min()
        arrival_time = df[df['baroaltitude'] == lowest_baroaltitude]['time'].iloc[0]

        features = entry_row[['lat', 'lon', 'velocity', 'heading', 'vertrate',
                            'baroaltitude', 'geoaltitude', 'hour']].to_dict()

        return {
            'callsign': entry_row['callsign'],
            'icao24': entry_row['icao24'],
            **features,
            'arrival_time': arrival_time,
            'transit_time': arrival_time - entry_row['time']
        }
    except Exception as e:
        logging.error(f"Error processing flight data: {e}")
        return None

# function to process parquet files
def process_parquet_files(parquet_files, airport_lat, airport_lon):
    """Process multiple parquet files with batch processing."""
    dataset = []
    flight_count = 0
    skipped_count = 0

    for file in parquet_files:
        df = pd.read_parquet(file)
        if df is None:
            continue

        for callsign, group in df.groupby('callsign'):
            result = process_flight_data(group, airport_lat, airport_lon)
            if result:
                dataset.append(result)
                flight_count += 1
            else:
                skipped_count += 1

        del df
        gc.collect()

    logging.info(f"Processed {flight_count} flights, skipped {skipped_count} flights")
    return pd.DataFrame(dataset)

# outlier filter
def filter_outliers(df):
    """Filter outliers based on domain-specific rules."""
    numeric_columns = ['baroaltitude', 'velocity', 'heading', 'vertrate']

    # Convert columns to numeric
    for col in numeric_columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # Convert transit_time to seconds
    df['transit_time'] = pd.to_timedelta(df['transit_time'].astype(str), errors='coerce').dt.total_seconds()

    # Convertire i valori problematici in NaN
    df['hour'] = df['hour'].apply(lambda x: x if isinstance(x, (str, pd.Timestamp, int, float)) else None)
    # Convertire la colonna in datetime
    df['hour'] = pd.to_datetime(df['hour'], errors='coerce')
    # Rimuovere il fuso orario, se presente
    df['hour'] = df['hour'].dt.tz_localize(None)


    # Apply filters
    filters = {
        'velocity': (100, 250),
        'baroaltitude': (3000, 7000),
        'vertrate': (-20, 5),
        'transit_time': (600, 3000)
    }

    for col, (min_val, max_val) in filters.items():
        mask = (df[col] > min_val) & (df[col] <= max_val)
        df = df[mask]

    return df

def create_model_pipeline():
    """Create sklearn pipeline for model training."""
    # Create a ColumnTransformer to handle numeric and categorical features separately
    numeric_features = ['lat', 'lon', 'velocity', 'heading', 'vertrate', 'baroaltitude', 'geoaltitude']
    categorical_features = ['icao24']

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', Pipeline([
                ('imputer', SimpleImputer(strategy='mean')),
                ('scaler', StandardScaler()),
            ]), numeric_features),
            ('cat', Pipeline([
                ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore')),
            ]), categorical_features),
        ])

    # Use the scikit-learn compatible version of XGBRegressor
    return Pipeline([
        ('preprocessor', preprocessor),
        ('model', lgb.LGBMRegressor(random_state=42, feature_types=None))
    ])

# main
if __name__ == "__main__":
    # mount Google Drive
    drive.mount('/content/drive')
    # folder path
    folder_path = '/content/drive/MyDrive/TESI/EGKK_202409/'
    # output file
    output_file = '/content/drive/MyDrive/TESI/flight_dataset_all_50.csv'

    # EGKK (Gatwick) airport coordinates
    airport_lat, airport_lon = 51.1537, -0.1821

    # get all the files in the folder
    parquet_files = glob.glob(folder_path + '*.parquet')

    # Process files
    final_result_df = process_parquet_files(parquet_files, airport_lat, airport_lon)
    # Save raw results
    final_result_df.to_csv(output_file, index=False)
    # Filter outliers
    final_result_df = filter_outliers(final_result_df)

    # split
    features = final_result_df.drop(['callsign', 'time', 'transit_time'], axis=1, errors='ignore')
    labels = final_result_df['transit_time']
    #
    # Rimuovi righe con valori NaN, infiniti o troppo grandi
    features.replace([np.inf, -np.inf], np.nan, inplace=True)
    labels.replace([np.inf, -np.inf], np.nan, inplace=True)

    valid_indices = features.notnull().all(axis=1) & labels.notnull()
    features = features[valid_indices]
    labels = labels[valid_indices]

    train_features, test_features, train_labels, test_labels = train_test_split(features,
                                                                                labels, test_size=0.25, random_state=42)

    # Define parameter grid
    param_grid = {
            'model__num_leaves': [31, 50, 100],
            'model__max_depth': [10, 20, 30],
            'model__learning_rate': [0.01, 0.1, 0.3],
            'model__n_estimators': [100, 500, 1000]
    }

    # Create and train model
    pipeline = create_model_pipeline()
    grid_search = GridSearchCV(
          pipeline,
          param_grid,
          cv=5,
          scoring='neg_mean_squared_error',
          n_jobs=-1,
          error_score='raise'
      )

    grid_search.fit(train_features, train_labels)

    # Make predictions
    predictions = grid_search.predict(test_features)

    # Calculate metrics
    scoring_metric = 'neg_mean_squared_error'
    cv_scores = cross_val_score(pipeline, features, labels, cv=5, scoring=scoring_metric)
    mean_cv_score = cv_scores.mean()
    print("Cross-validation:", mean_cv_score)
    #
    print('Best Parameters:', grid_search.best_params_)
    print('Mean Absolute Error:', mean_absolute_error(test_labels, predictions))
    print('Mean Squared Error:', mean_squared_error(test_labels, predictions))
    print('Root Mean Squared Error:', np.sqrt(mean_squared_error(test_labels, predictions)))
    print('R^2 Score:', r2_score(test_labels, predictions))

    # Calculate MAPE
    non_zero_mask = test_labels != 0
    mape = np.mean(np.abs((test_labels[non_zero_mask] - predictions[non_zero_mask])
                          / test_labels[non_zero_mask])) * 100
    accuracy = 100 - np.mean(mape)
    print('Accuracy:', round(accuracy, 2), '%.')

    # GRAPHS
    # feature importance
    best_model = grid_search.best_estimator_.named_steps['model']
    numeric_features = ['lat', 'lon', 'velocity', 'heading', 'vertrate', 'baroaltitude', 'geoaltitude']
    categorical_features = ['icao24']
    feature_names = numeric_features + list(grid_search.best_estimator_
                                    .named_steps['preprocessor']
                                    .named_transformers_['cat']
                                    .named_steps['onehot']
                                    .get_feature_names_out(categorical_features))
    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': best_model.feature_importances_})
    importance_df = importance_df.sort_values(by='Importance', ascending=False)

    plt.figure(figsize=(12, 6))
    sns.barplot(x='Importance', y='Feature', data=importance_df.nlargest(7, 'Importance'))
    plt.title('Feature Importances')
    plt.show()

    # scatter plot: prediction vs actual value
    plt.figure(figsize=(12, 6))
    plt.scatter(test_labels, predictions, alpha=0.3)
    plt.plot([test_labels.min(), test_labels.max()], [test_labels.min(), test_labels.max()], 'r--')
    plt.xlabel('Valori Reali')
    plt.ylabel('Valori Previsti')
    plt.title('Valori Reali vs Valori Previsti')
    plt.show()

    # scatter plot: prediction errors
    errors = predictions - test_labels
    plt.figure(figsize=(12, 6))
    sns.histplot(errors, kde=True, bins=30)
    plt.title('Distribuzione degli Errori di Previsione')
    plt.xlabel('Errore')
    plt.ylabel('Frequenza')
    plt.show()

    def plot_the_model(model, X, y):
        # Apply preprocessing steps to X before prediction
        # Use the entire pipeline for transformation, not just the 'model' step
        # FIXED: Call transform on the pipeline created outside the function (main pipeline)
        X_transformed = grid_search.best_estimator_.named_steps['preprocessor'].transform(X)
        # Convert the transformed data back into a DataFrame to make it plottable
        X_transformed = pd.DataFrame(X_transformed, columns=feature_names)
        plt.figure(figsize=(10, 6))
        plt.scatter(X_transformed.iloc[:, 0], y, color='blue', label='Valori Reali')
        # FIXED: Use the entire pipeline for prediction
        plt.plot(X_transformed.iloc[:, 0], grid_search.best_estimator_.predict(X), color='red', linewidth=2, label='Predizioni del modello')
        plt.xlabel('Prima caratteristica (scalata)')
        plt.ylabel('Tempo')
        plt.legend()
        plt.show()

    # loss curve
    def plot_the_loss_curve(y_true, y_pred):
        errors = y_true - y_pred
        plt.figure(figsize=(10, 6))
        plt.hist(errors, bins=50)
        plt.xlabel('Errore di Predizione')
        plt.ylabel('Frequenza')
        plt.show()

    plot_the_model(best_model, test_features, test_labels)
    plot_the_loss_curve(test_labels, predictions)

    # boxplot: prediction errors
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=errors)
    plt.title('Distribuzione degli Errori di Previsione')
    plt.xlabel('Errore')
    plt.show()

    # violin plot: prediction errors
    plt.figure(figsize=(10, 6))
    sns.violinplot(x=errors)
    plt.title('Distribuzione degli Errori di Previsione (Violin Plot)')
    plt.xlabel('Errore')
    plt.show()

    # scatter plot: velocity wise
    plt.figure(figsize=(10, 6))
    plt.scatter(test_features.iloc[:, 2], errors, alpha=0.5)  # test_features[:, 2] è la velocità
    plt.xlabel('Velocità')
    plt.ylabel('Errore di Predizione')
    plt.title('Errore vs Velocità')
    plt.show()

    # scatter plot: baroaltitude wise
    plt.figure(figsize=(10, 6))
    plt.scatter(test_features.iloc[:, 5], errors, alpha=0.5)  # test_features[:, 5] è baroaltitude
    plt.xlabel('Altitudine Barometrica')
    plt.ylabel('Errore di Predizione')
    plt.title('Errore vs Altitudine Barometrica')
    plt.show()

    # features correlation heatmap
    plt.figure(figsize=(12, 8))
    correlation_matrix = pd.DataFrame(train_features, columns=['lat', 'lon',
                        'velocity', 'heading', 'vertrate', 'baroaltitude', 'geoaltitude']).corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
    plt.title('Matrice di Correlazione delle Feature')
    plt.show()

     # residual graph
     plt.figure(figsize=(10, 6))
     sns.residplot(x=test_labels, y=errors, lowess=True, color="blue", line_kws={'color': 'red', 'lw': 1})
     plt.title('Residual Plot')
     plt.xlabel('Valori Reali')
     plt.ylabel('Residui')
     plt.show()

     # performance in cross-validation
     plt.figure(figsize=(10, 6))
     plt.plot(range(1, len(cv_scores) + 1), -cv_scores, marker='o', linestyle='--', color='b', label='CV MAE')
     plt.xlabel('Fold')
     plt.ylabel('MAE')
     plt.title('Performance del modello durante la Cross-Validation')
     plt.legend()
     plt.show()

     # Istogrammi per analizzare le distribuzioni
     numeric_cols = ['baroaltitude', 'velocity', 'heading', 'vertrate']
     for col in numeric_cols:
        plt.figure(figsize=(8, 4))
        sns.histplot(final_result_df[col], bins=50, kde=True)
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Frequency')
        plt.show()
